{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.01 - Supervised Learning Model Comparison\n",
    "\n",
    "Recall the \"data science process.\"\n",
    "\n",
    "1. Define the problem.\n",
    "2. Gather the data.\n",
    "3. Explore the data.\n",
    "4. Model the data.\n",
    "5. Evaluate the model.\n",
    "6. Answer the problem.\n",
    "\n",
    "In this lab, we're going to focus mostly on creating (and then comparing) many regression and classification models. Thus, we'll define the problem and gather the data for you.\n",
    "Most of the questions requiring a written response can be written in 2-3 sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define the problem.\n",
    "\n",
    "You are a data scientist with a financial services company. Specifically, you want to leverage data in order to identify potential customers.\n",
    "\n",
    "If you are unfamiliar with \"401(k)s\" or \"IRAs,\" these are two types of retirement accounts. Very broadly speaking:\n",
    "- You can put money for retirement into both of these accounts.\n",
    "- The money in these accounts gets invested and hopefully has a lot more money in it when you retire.\n",
    "- These are a little different from regular bank accounts in that there are certain tax benefits to these accounts. Also, employers frequently match money that you put into a 401k.\n",
    "- If you want to learn more about them, check out [this site](https://www.nerdwallet.com/article/ira-vs-401k-retirement-accounts).\n",
    "\n",
    "We will tackle one regression problem and one classification problem today.\n",
    "- Regression: What features best predict one's income?\n",
    "- Classification: Predict whether or not one is eligible for a 401k.\n",
    "\n",
    "Check out the data dictionary [here](http://fmwww.bc.edu/ec-p/data/wooldridge2k/401KSUBS.DES).\n",
    "\n",
    "### NOTE: When predicting `inc`, you should pretend as though you do not have access to the `e401k`, the `p401k` variable, and the `pira` variable. When predicting `e401k`, you may use the entire dataframe if you wish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Gather the data.\n",
    "\n",
    "##### 1. Read in the data from the repository.\n",
    "\n",
    "    - a multiple linear regression model\n",
    "    - a k-nearest neighbors model\n",
    "    - a decision tree\n",
    "    - a set of bagged decision trees\n",
    "    - a random forest\n",
    "    - an Adaboost model\n",
    "    - a support vector regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor, AdaBoostRegressor\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e401k</th>\n",
       "      <th>inc</th>\n",
       "      <th>marr</th>\n",
       "      <th>male</th>\n",
       "      <th>age</th>\n",
       "      <th>fsize</th>\n",
       "      <th>nettfa</th>\n",
       "      <th>p401k</th>\n",
       "      <th>pira</th>\n",
       "      <th>incsq</th>\n",
       "      <th>agesq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>13.170</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>4.575</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>173.4489</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>61.230</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>154.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3749.1130</td>\n",
       "      <td>1225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>12.858</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>165.3282</td>\n",
       "      <td>1936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>98.880</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>21.800</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9777.2540</td>\n",
       "      <td>1936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>22.614</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>18.450</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>511.3930</td>\n",
       "      <td>2809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9270</th>\n",
       "      <td>0</td>\n",
       "      <td>58.428</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.200</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3413.8310</td>\n",
       "      <td>1089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9271</th>\n",
       "      <td>0</td>\n",
       "      <td>24.546</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>3</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>602.5061</td>\n",
       "      <td>1369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9272</th>\n",
       "      <td>0</td>\n",
       "      <td>38.550</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "      <td>-13.600</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1486.1020</td>\n",
       "      <td>1089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9273</th>\n",
       "      <td>0</td>\n",
       "      <td>34.410</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>3</td>\n",
       "      <td>3.550</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1184.0480</td>\n",
       "      <td>3249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9274</th>\n",
       "      <td>0</td>\n",
       "      <td>25.608</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>1.800</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>655.7697</td>\n",
       "      <td>2401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9275 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      e401k     inc  marr  male  age  fsize   nettfa  p401k  pira      incsq  \\\n",
       "0         0  13.170     0     0   40      1    4.575      0     1   173.4489   \n",
       "1         1  61.230     0     1   35      1  154.000      1     0  3749.1130   \n",
       "2         0  12.858     1     0   44      2    0.000      0     0   165.3282   \n",
       "3         0  98.880     1     1   44      2   21.800      0     0  9777.2540   \n",
       "4         0  22.614     0     0   53      1   18.450      0     0   511.3930   \n",
       "...     ...     ...   ...   ...  ...    ...      ...    ...   ...        ...   \n",
       "9270      0  58.428     1     0   33      4   -1.200      0     0  3413.8310   \n",
       "9271      0  24.546     0     1   37      3    2.000      0     0   602.5061   \n",
       "9272      0  38.550     1     0   33      3  -13.600      0     1  1486.1020   \n",
       "9273      0  34.410     1     0   57      3    3.550      0     0  1184.0480   \n",
       "9274      0  25.608     0     1   49      1    1.800      0     0   655.7697   \n",
       "\n",
       "      agesq  \n",
       "0      1600  \n",
       "1      1225  \n",
       "2      1936  \n",
       "3      1936  \n",
       "4      2809  \n",
       "...     ...  \n",
       "9270   1089  \n",
       "9271   1369  \n",
       "9272   1089  \n",
       "9273   3249  \n",
       "9274   2401  \n",
       "\n",
       "[9275 rows x 11 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('401ksubs.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. What are 2-3 other variables that, if available, would be helpful to have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Suppose a peer recommended putting `race` into your model in order to better predict who to target when advertising IRAs and 401(k)s. Why would this be an unethical decision?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is racially bias. not cool. Giving unequal opportunities based on race is unethical. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Explore the data.\n",
    "\n",
    "##### 4. When attempting to predict income, which feature(s) in our dataset would we reasonably not use? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer :\n",
    "# Marriage, Sex, Age, Family Size, Net Total,\n",
    "# and reasonably I would test it if they\n",
    "# already have a 401k or IRA but we are to pretend we do not. \n",
    "# These all may have an effect on their income.\n",
    "# Just removing the income since we are looking for that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. What two variables have already been created for us through feature engineering? Come up with a hypothesis as to why subject-matter experts may have done this.\n",
    "> This need not be a \"statistical hypothesis.\" Just brainstorm why SMEs might have done this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Income and age are squared. This may be so we have a larger range \n",
    "# and thus our samples are more spread out, \n",
    "# giving us less bias in our answers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Looking at the data dictionary, two variable descriptions appear to be errors. What are these errors, and what do you think the correct value would be, looking at the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# income and age are labeled as if they are squared but they are not in that column. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model the data. (Part 1: Regression Problem)\n",
    "\n",
    "Recall:\n",
    "- Problem: What features best predict one's income?\n",
    "- When predicting `inc`, you should pretend as though you do not have access to the `e401k`, the `p401k` variable, and the `pira` variable.\n",
    "\n",
    "##### 7. List all models/modeling tactics we've learned that could be used to solve a regression problem (as of Wednesday afternoon of Week 6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logerithmic regression, LASSO, Linear Regression, KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. Regardless of your answer to number 7, fit at least one of each of the following models to attempt to solve the regression problem above. You will be asked to evaluate your models later in Step 5:\n",
    "    - a multiple linear regression model\n",
    "    - a k-nearest neighbors model\n",
    "    - a decision tree\n",
    "    - a set of bagged decision trees\n",
    "    - a random forest\n",
    "    - an Adaboost model\n",
    "    - a support vector regressor\n",
    "    \n",
    "> As always, be sure to do a train/test split! In order to compare modeling techniques, you should use the same train-test split on each. I recommend setting a random seed here.\n",
    "\n",
    "> You may find it helpful to set up a pipeline to try each modeling technique, but you are not required to do so!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.363413714273026\n",
      "20.185058402131137\n",
      "27.2474278240084\n",
      "20.78811812170192\n",
      "20.33174009622726\n",
      "21.588168038344634\n",
      "20.467288297315545\n"
     ]
    }
   ],
   "source": [
    "features = ['marr','male', 'age', 'fsize', 'nettfa']\n",
    "X = df[features]\n",
    "y = df['inc']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    random_state =42, \n",
    "                                                    test_size = 0.2)\n",
    "\n",
    "lr = LinearRegression()\n",
    "knn = KNeighborsRegressor()\n",
    "dt = DecisionTreeRegressor()\n",
    "bag = BaggingRegressor()\n",
    "rf = RandomForestRegressor()\n",
    "adboost = AdaBoostRegressor()\n",
    "svr = SVR()\n",
    "\n",
    "est = [lr, knn, dt, bag, rf, adboost, svr]\n",
    "\n",
    "mse = mean_squared_error\n",
    "\n",
    "for model in est:\n",
    "    pipe = Pipeline([\n",
    "        ('sc', StandardScaler()), \n",
    "        ('estimator', model) ])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    print(mse(y_test, y_pred, squared = False))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9. What is bootstrapping?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking samples with replacement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10. What is the difference between a decision tree and a set of bagged decision trees? Be specific and precise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "A decision tree:\n",
    "- takes a dataset consisting of $X$ and $Y$ data, \n",
    "- finds rules based on our $X$ data that partitions (splits) our data into smaller datasets such that\n",
    "- by the bottom of the tree, the values $Y$ in each \"leaf node\" are as \"pure\" as possible.\n",
    "\n",
    "    Decision trees have some limitations. In particular, trees that are grown very deep tend to learn highly irregular patterns (a.k.a. they overfit their training sets). \n",
    "\n",
    "Bagging (bootstrap aggregating):\n",
    "           Mitigates this problem by exposing different trees(random rows) to different sub-samples of the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 11. What is the difference between a set of bagged decision trees and a random forest? Be specific and precise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "    forest is random columns or features. \n",
    "    trees is random rows or differeng samples of data. \n",
    "\n",
    "    Random forests differ from bagging decision trees in only one way: they use a modified tree learning algorithm that selects, at each split in the learning process, a **random subset of the features**. This process is sometimes called the *random subspace method*.\n",
    "\n",
    "    The reason for doing this is the correlation of the trees in an ordinary bootstrap sample: if one or a few features are very strong predictors for the response variable (target output), these features will be used in many/all of the bagged decision trees, causing them to become correlated. By selecting a random subset of features at each split, we counter this correlation between base trees, strengthening the overall model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 12. Why might a random forest be superior to a set of bagged decision trees?\n",
    "> Hint: Consider the bias-variance tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "    By \"de-correlating\" our trees from one another, we can drastically reduce the variance of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the model. (Part 1: Regression Problem)\n",
    "\n",
    "##### 13. Using RMSE, evaluate each of the models you fit on both the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression() RMSE for the testing data:\n",
      "21.363413714273026\n",
      "\n",
      "KNeighborsRegressor() RMSE for the testing data:\n",
      "20.185058402131137\n",
      "\n",
      "DecisionTreeRegressor() RMSE for the testing data:\n",
      "27.21956660713901\n",
      "\n",
      "BaggingRegressor() RMSE for the testing data:\n",
      "20.838286895492555\n",
      "\n",
      "RandomForestRegressor() RMSE for the testing data:\n",
      "20.211332260682823\n",
      "\n",
      "AdaBoostRegressor() RMSE for the testing data:\n",
      "23.543041422081203\n",
      "\n",
      "SVR() RMSE for the testing data:\n",
      "20.467288297315545\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in est:\n",
    "    pipe = Pipeline([\n",
    "        ('sc', StandardScaler()), \n",
    "        ('estimator', model) ])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    print(f'{model} RMSE for the testing data:')\n",
    "    print(mse(y_test, y_pred, squared = False))\n",
    "    print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression() RMSE for the testing data:\n",
      "20.497257487705625\n",
      " \n",
      "KNeighborsRegressor() RMSE for the testing data:\n",
      "16.483877636883296\n",
      " \n",
      "DecisionTreeRegressor() RMSE for the testing data:\n",
      "2.2638130048030134\n",
      " \n",
      "BaggingRegressor() RMSE for the testing data:\n",
      "8.85885824420104\n",
      " \n",
      "RandomForestRegressor() RMSE for the testing data:\n",
      "7.701698802675366\n",
      " \n",
      "AdaBoostRegressor() RMSE for the testing data:\n",
      "22.548451357765014\n",
      " \n",
      "SVR() RMSE for the testing data:\n",
      "19.778944149412197\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for model in est:\n",
    "    pipe = Pipeline([\n",
    "        ('sc', StandardScaler()), \n",
    "        ('estimator', model) ])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_train)\n",
    "    print(f'{model} RMSE for the training data:')\n",
    "    print(mse(y_train, y_pred, squared = False))\n",
    "    print(' ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 14. Based on training RMSE and testing RMSE, is there evidence of overfitting in any of your models? Which ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "    Yes there is certianally some overfitting going on. \n",
    "    The RMSE scores:\n",
    "        -KNN has a net gain of about 6\n",
    "        -Decision Tree is incredibly overfit with a gain of close to 35\n",
    "        -Bagging has a gain of 17\n",
    "        -Random forest a gain of 17\n",
    "        \n",
    "    Ada boost has a loss (underfitting) of about 5, yet the RMSE is already far higher it is likely not a good model in general for this matter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 15. Based on everything we've covered so far, if you had to pick just one model as your final model to use to answer the problem in front of you, which one model would you pick? Defend your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "    Linear Regression because the model has the least about of change from the test to the training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 16. Suppose you wanted to improve the performance of your final model. Brainstorm 2-3 things that, if you had more time, you would attempt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I would do a gridsearch to find better parameters on my models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Model the data. (Part 2: Classification Problem)\n",
    "\n",
    "Recall:\n",
    "- Problem: Predict whether or not one is eligible for a 401k.\n",
    "- When predicting `e401k`, you may use the entire dataframe if you wish.\n",
    "\n",
    "##### 17. While you're allowed to use every variable in your dataframe, mention at least one disadvantage of using `p401k` in your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOr a targeted marketing plan, it makes little sence to seek\n",
    "# out people who already have 401k's. They likely cannot switch over\n",
    "# or are already investing in their own 401k. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 18. List all modeling tactics we've learned that could be used to solve a classification problem (as of Wednesday afternoon of Week 6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 19. Regardless of your answer to number 18, fit at least one of each of the following models to attempt to solve the classification problem above. You will be asked to evaluate your models later in Step 5:\n",
    "    - a logistic regression model\n",
    "    - a k-nearest neighbors model\n",
    "    - a decision tree\n",
    "    - a set of bagged decision trees\n",
    "    - a random forest\n",
    "    - an Adaboost model\n",
    "    - a support vector classifier\n",
    "    \n",
    "> As always, be sure to do a train/test split! In order to compare modeling techniques, you should use the same train-test split on each. I recommend using a random seed here.\n",
    "\n",
    "> You may find it helpful to set up a pipeline to try each modeling technique, but you are not required to do so!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5936546774100089: train: LogisticRegression()\n",
      "0.5785938899503439: test: LogisticRegression()\n",
      "\n",
      "0.5008079725173384: train: KNeighborsClassifier()\n",
      "0.6018838350906229: test: KNeighborsClassifier()\n",
      "\n",
      "0.0: train: DecisionTreeClassifier()\n",
      "0.6337327912250197: test: DecisionTreeClassifier()\n",
      "\n",
      "0.1509181245690859: train: BaggingClassifier()\n",
      "0.6054558773412049: test: BaggingClassifier()\n",
      "\n",
      "0.0: train: RandomForestClassifier()\n",
      "0.5928595784706245: test: RandomForestClassifier()\n",
      "\n",
      "0.5578404644945482: train: AdaBoostClassifier()\n",
      "0.5620524574802326: test: AdaBoostClassifier()\n",
      "\n",
      "0.5678967675066257: train: SVC()\n",
      "0.5692005080187833: test: SVC()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features = ['marr','male', 'age', 'fsize', 'nettfa', 'inc']\n",
    "X = df[features]\n",
    "y = df['e401k']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    random_state =42, \n",
    "                                                    test_size = 0.2)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "knnc = KNeighborsClassifier()\n",
    "dtc = DecisionTreeClassifier()\n",
    "bagc = BaggingClassifier()\n",
    "rfc = RandomForestClassifier()\n",
    "adboostc = AdaBoostClassifier()\n",
    "svc = SVC()\n",
    "\n",
    "est = [logreg, knnc, dtc, bagc, rfc, adboostc, svc]\n",
    "\n",
    "mse = mean_squared_error\n",
    "\n",
    "for model in est:\n",
    "    pipe = Pipeline([\n",
    "        ('sc', StandardScaler()), \n",
    "        ('estimator', model) ])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred_train = pipe.predict(X_train)\n",
    "    y_pred_test = pipe.predict(X_test)\n",
    "    \n",
    "    print(f'{mse(y_train, y_pred_train, squared = False)}: train: {model}')\n",
    "    print(f'{mse(y_test, y_pred_test, squared = False)}: test: {model}')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the model. (Part 2: Classfication Problem)\n",
    "\n",
    "##### 20. Suppose our \"positive\" class is that someone is eligible for a 401(k). What are our false positives? What are our false negatives?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A false positive is someone who comes up as eligable but they are not in actuality. \n",
    "# A false negative is someone who comes up as ineligable but they do qualify for a 401k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 21. In this specific case, would we rather minimize false positives or minimize false negatives? Defend your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "    Minimizing false negatives are better for the financial aspects of the company because they will interact with more people than they miss out on. However having both low is the best option. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 22. Suppose we wanted to optimize for (minimize) the answer you provided in problem 21. Which metric would we optimize (maximize) in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    To minimize false negatives and not be so concerened about false positives, we would want increase the sensitivity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 23. Suppose that instead of optimizing for the metric in problem 21, we wanted to balance our false positives and false negatives using `f1-score`. Why might [f1-score](https://en.wikipedia.org/wiki/F1_score) be an appropriate metric to use here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "    The f1-score is optimized when there is high precision and lower false (negatives and positives)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 24. Using f1-score, evaluate each of the models you fit on both the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3745515426931356: train: LogisticRegression()\n",
      "0.4057416267942584: test: LogisticRegression()\n",
      "\n",
      "0.6516938049784766: train: KNeighborsClassifier()\n",
      "0.4947368421052631: test: KNeighborsClassifier()\n",
      "\n",
      "1.0: train: DecisionTreeClassifier()\n",
      "0.4917582417582418: test: DecisionTreeClassifier()\n",
      "\n",
      "0.9690467815687653: train: BaggingClassifier()\n",
      "0.46993670886075944: test: BaggingClassifier()\n",
      "\n",
      "1.0: train: RandomForestClassifier()\n",
      "0.5182370820668692: test: RandomForestClassifier()\n",
      "\n",
      "0.5639282341831916: train: AdaBoostClassifier()\n",
      "0.5593984962406015: test: AdaBoostClassifier()\n",
      "\n",
      "0.4495054060271451: train: SVC()\n",
      "0.44300278035217794: test: SVC()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in est:\n",
    "    pipe = Pipeline([\n",
    "        ('sc', StandardScaler()), \n",
    "        ('estimator', model) ])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred_train = pipe.predict(X_train)\n",
    "    y_pred_test = pipe.predict(X_test)\n",
    "    \n",
    "    print(f'{f1_score(y_train, y_pred_train)}: train: {model}')\n",
    "    print(f'{f1_score(y_test, y_pred_test)}: test: {model}')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 25. Based on training f1-score and testing f1-score, is there evidence of overfitting in any of your models? Which ones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "    There is evidence of overfitting. KNN, DecisionTree, BaggingClassifier, RandomForest are all overfit. Remarkable though I must add that Random Forest has a 100% fscore on training and the second highest performing fscore on the training data at 52%. Not bad for being incredibly overfit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 26. Based on everything we've covered so far, if you had to pick just one model as your final model to use to answer the problem in front of you, which one model would you pick? Defend your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "    AdaBoostClassifier. It is not overfit or underfit with the highest testing fscore. This indicates that it is a highly effective model compared to the others presented here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 27. Suppose you wanted to improve the performance of your final model. Brainstorm 2-3 things that, if you had more time, you would attempt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "    Gridsearch other parameters and determine which models have the least variance and bias effecting their model. \n",
    "    Determine the coeeficients and see how much of an effect each variable has on the model to see if I could tweak the model some more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Answer the problem. [BONUS] \n",
    "\n",
    "##### BONUS: Briefly summarize your answers to the regression and classification problems. Be sure to include any limitations or hesitations in your answer.\n",
    "\n",
    "- Regression: What features best predict one's income?\n",
    "- Classification: Predict whether or not one is eligible for a 401k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
